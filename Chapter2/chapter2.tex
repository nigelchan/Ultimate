\chapter{Uniform Bounds for Functionals of Non-stationary Time Series}
\ifpdf
    \graphicspath{{Chapter2/Chapter2Figs/PNG/}{Chapter2/Chapter2Figs/PDF/}{Chapter2/Chapter2Figs/}}
\else
    \graphicspath{{Chapter2/Chapter2Figs/EPS/}{Chapter2/Chapter2Figs/}}
\fi

\section{Introduction}
Consider a triangular array ${x_{k,n},1\leq k\leq n,n\geq
1}$ constructed from some underlying time series.
In most practical situations, $x_{k,n}$ is equal to $x_k/d_n$, where $x_k$
is a partial sum and $0 < d_n\to \infty$ in such a way that $x_n/d_n$ has a limit distribution.  The functional of interest $
S_{n}$ of $x_{k,n}$ is defined by the sample average
\bestar
S_{n} (x)=\sum_{k=1}^{n}g[c_{n}\,(x_{k,n}+x)], \quad x\in R,
\eestar
where $c_{n}$ is a certain sequence of positive constants and $g$ is
a real function on $R$. Such functionals commonly arise in
non-linear regression with integrated time series [\citet[][\citeyear{parkphillips2001}]{parkphillips1999}] and non-parametric estimation in relation to nonlinear cointegration models [\cite{phillipspark1998}, \cite{karlsentjostheim2001}, \citet[][\citeyear{wangphillips2010a}, \citeyear{wangphillips2010b}]{wangphillips2009}]. The limit
behavior of $S_{n}(x)$ in the situation that $c_{n}\rightarrow \infty $ and $%
n/c_{n}\rightarrow \infty $ is particularly interesting and
important for practical applications as it provides a setting that
accommodates a sufficiently wide range of bandwidth choices to be
relevant for non-parametric kernel estimation.

For a fixed $x$, the limit distribution of $S_n(x)$ has been established by \citet[][\citeyear{wangphillips2010a}, \citeyear{wangphillips2010b}]{wangphillips2009} under very general setting on $x_{k,n}$. The aim of this section is investigate the uniform (upper and lower) bound  for $S_n(x)$ on a compact set or on $R$. As discussed in {\bf \color{red} Section 2}, these results will be useful in the investigation of uniform convergence for kernel estimates in a non-linear cointegrating regression. 


\section{Main results}

We make use of the following assumptions in the development of main results.

\medskip
\begin{assump} \la{assump:2:lipschitz}
	$\sup_x|g(x)|<\infty$, $\int_{-\infty}^{\infty}|g(x)|dx<\infty$ \textit{and }\textit{\
	$|g(x)-g(y)|\le C|x-y|$ whenever  $|x-y|$ is sufficient small on }$R$.
\end{assump}

\medskip
\begin{assump} \la{assump:2:weakConvergence}
\textit{There exists a stochastic process }%
$G(t)$\textit{\ having a continuous local time
}$L_{G}(t,s)$\textit{\ such that }$x_{[nt],n}\Rightarrow
G(t)$\textit{, on }$D[0,1],$\textit{\ where
weak convergence is understood w.r.t the Skorohod topology on the space }$%
D[0,1]$\textit{.}
\end{assump}

\medskip
\begin{assump} \la{assump:2:density}
 \textit{For all }$0\leq k<l\leq n,n\geq 1$ \textit{, there exist a sequence of constants }$d_{l,k,n}\sim C_0 [n/(l-k)]^{-d}$\textit{\ for some $0< d<1$
and a
sequence of increasing }$\sigma $\textit{-fields }${\mathcal F}_{k,n}$\textit{\ (define }$%
{\mathcal F}_{0,n}=\sigma \{\phi ,\Omega \}$\textit{, the trivial }$\sigma $\textit{%
-field) such that} $x_{k,n}$\textit{\ are adapted to }${\mathcal F}_{k,n}$\textit{\
and, conditional on }${\mathcal F}_{k,n}$\textit{,
}$(x_{l,n}-x_{k,n})/d_{l,k,n}$\textit{\ has a density
}$h_{l,k,n}(x)$\textit{\ satisfying that }$h_{l,k,n}(x)$\textit{\ is
uniformly bounded by a constant }$K$\textit{\ and uniformly for $j-k$ sufficiently large}
\be
 \sup_y | h_{l,k,n}(y + u) - h_{l,k,n}(y)| \le C \min\{|u|, 1\}. \la {eqn:2:77}
\ee
\end{assump}

We have the following theorem.

\begin{thm} \la {thmMainUpper}  Under Assumptions \ref{assump:2:lipschitz} and \ref{assump:2:density}, we have
\be \la{thmMainUpper.eqn1}
\sup_{|x|\le n^{m_0}} |S_n(x)| =O[(n/c_n)\log n],\quad a.s., \la {eqn:2:ad1}
\ee
 for any $c_n\to\infty$ and $c_n/n\to 0$, and any fixed constant $0<m_0<\infty$. If there exist positive constants  $m$ (allow to be sufficient large) and
 $k$ (allow to be sufficient small) such that $n\sup_{|x|>n^{m}/2}|g(c_n\, x)|=O[(n/c_n)\log n]$  and $
n^{-m k} \sum_{t=1}^n |x_{t,n}|^k  = O(1)\, a.s.,
$
then
\be
\sup_{x\in R} |S_n(x)| =O[(n/c_n)\log n], \quad a.s. \la {eqn:2:69}
\ee
\end{thm}


We mention that the additional conditions to establish (\ref {eqn:2:69}) are close to minimal. The bound can be improved if we are only concerned with
convergence in probability, as stated in the following theorem.

\begin{thm} \la{thm:2:MainLower}  Under Assumptions \ref{assump:2:lipschitz} and \ref{assump:2:density},  we have
\be
\sup_{|x|\le M_0/ \log^{\gamma_1} n} |S_n(x)| =O_P(n/c_n),\quad  \la {eqn:2:ad12}
\ee
where
% \be
% \gamma_1 &=&\begin{cases}
% 4\,\big (\frac{d}{1-d}\big),  & \quad if \quad  0 < d \le 3/5, \\
%\big ( \frac{1+d}{1-d} \big ) \big ( \frac{d}{1-d} \big ), &\quad if \quad 3/5<d <1.
%\end{cases} \la {eqn:2:gamma}
%\ee
for any fixed $M_0>0$, $c_n\to\infty$ and $(n/c_n) \log^{-\theta}n \to \infty$, where $\theta = (1-d)\gamma_1/d$. If in addition  $\int_{-\infty}^{\infty} g(x)dx\not=0$,  then
\be \la{eqn:2:MainLower1}
\Big [ \inf_{|x|\le M_0/ \log^{\gamma_1} n}| S_n(x)|\Big]^{-1} =\sup_{|x|\le M_0/ \log^{\gamma_1} n} |S_n^{-1}(x)| =O_P(c_n/n) .
\ee
\end{thm}


\begin{rem}  The requirement on $d_{l,k,n}$ is mild. Indeed, in  most practical situations, $x_{k,n}=\sum_{j=1}^k \eta_j/d_n$, where $d_n^2= var (\sum_{j=1}^k\eta_j)\sim C_0n^{2d}$ for some $0< d<1$, as stated in the following examples. It is hence natural to assume $d_{l,k,n}\sim C_0 [n/(l-k)]^{-d}$. This condition can  be generalized to $d_{l,k,n}\sim C_0 [n\rho (n)/(l-k) \rho(l-k)]^{-d}$, where $\rho (n)$ is slowly varying function at infinity or more generally to those as in Assumption 2.3 (i) of \cite{wangphillips2010a} without essential difficulty. We omit this kind of generalization here for notation convenience.
\end{rem}

\begin{rem} It is readily seen that $d_{l,k,n}\sim C_0 [n/(l-k)]^{-d}$ for some $0< d<1$ satisfies Assumption 2.3 (i) of \cite{wangphillips2010a}.
If in addition to Assumptions \ref{assump:2:lipschitz} and \ref{assump:2:density}, $\int_{\infty}^{\infty} g(x)dx\not=0$.  Theorem 2.1 and Remark 2.1 of \cite{wangphillips2010a} yield that
\be
\frac{c_n}{n}\sum_{t = 1}^{n} g[c_n\, (x_{t,n}+y_n)] \to_D \int_{-\infty}^{\infty} g(x)dx\, L_G(1,y), \la {eqn:2:99}
\ee
whenever $c_n\to\infty$, $n/c_n\to\infty$ and $y_n\to y$,
where $L_G(1, 0)$ is the local time process defined by
\bestar
L_G(t,s) = \lim_{\ep \to 0} \frac{1}{2\ep} \int_0^t I \big \{ | G(r) - s| \le \ep \big \} dr.
\eestar
Note that $P(0<L_G(1, x) <\infty)=1$, for any fixed $x\in R$ and $L_G(1, x)\to 0$, as $x\to \infty$.
It might be possible to improve the range $|x|\le M_0/ \log^{\gamma_1} n$ in  (\ref {eqn:2:ad12}) and (\ref {eqn:2:MainLower1}) into $x\in R$ and
 $|x|\le M_0$ (not possible for $|x|\le b_n$ in (\ref {eqn:2:MainLower1}) where $b_n\to\infty$), respectively. However, to do this, we  require a quite different technique, and hence leave it for future work.
\end{rem}

The essential behind the proof of Theorem \ref {thm:2:MainLower} is a fact   that $S_n(x)$ can be approximated by $S_n(0)$ under a reasonable rate. Explicitly we have the following theorem.


\begin{thm} \la{thm:2:thm7} Let $\gamma \ge 0$. Under Assumptions \ref{assump:2:lipschitz} and \ref{assump:2:density},  we have
\be
\sup_{|x|\le M_0/ \log^\gamma n} |S_n(x)-S_n(0)| =O_P[(n/c_n) \log^{1-\lambda} n],\quad  \la {eqn:2:ad121}
\ee
 for any fixed $M_0>0$, $c_n\to\infty$ and $(n/c_n) \log^{-\theta}n \to \infty$, where $\theta=\max\{\lambda+1, \eta\}$,
\be
\lambda = \begin{cases}
\frac{(1-d)^2\gamma/d-(2d-1)}{2-d},  & \mbox{ if    $1/2 < d < 1$  and  $\gamma \ge d(5d-1)/(2d-1)$}, \\
\frac{(1+\gamma)(1-d)-2d}{1+d},  & otherwise, \\
\end{cases}
\ee
 and
\be \la{eta}
\eta = \begin{cases}
\gamma + (\lambda+1) (1-2d)/(1-d),  & if \quad   0 < d < 1/2, \\
\gamma -1, & if \quad  d = 1/2, \\
(1-d)\gamma / d, & if \quad  1/2 < d < 1.
\end{cases}
\ee
\end{thm}





To end this section, we introduce the following
 examples on $x_{k,n}$, which satisfy Assumptions \ref{assump:2:weakConvergence} and \ref{assump:2:density}.

\medskip
{\bf Example 3.1.} Let $\{ \xi_j, j \ge 1\}$ be a stationary sequence of Gaussian random variables  with $\E \xi_1 = 0$ and covariances $\gamma( j - i) = \E \xi_j \xi_i$ satisfying the following condition for some $0 < \beta < 2$ and $\lambda > 1$,
\be
d_n^2 \equiv \sum_{1 \le i,j\le n} \gamma(j-i) \sim n^\beta \quad  and \quad  | \tilde{\gamma}_{l,k} | \le \lambda d_k d_{l-k},
\ee
as $\min \{k, l-k\} \to \infty$, where
\be
\tilde{\gamma}_{l,k} = \sum_{i = 1}^k \sum_{j = k+1}^l \gamma(j-i)
\ee
Let $x_{k,n}= \sum_{j = 1}^k \xi_j/d_n$, $1 \le k \le n$. Then $x_{k,n}$ satisfies Assumptions \ref{assump:2:weakConvergence} and \ref{assump:2:density} with $G(t) = W_{\beta/2}(t)$.   See Corollary 2.1 of \cite{wangphillips2010a}.

Here and below, $W_{\beta}(t)$ denotes  fractional Brownian motion with $0<\beta<1$ on
$D[0,1]$, defined as follows:
\begin{eqnarray*}
W_{\beta}(t)=\frac 1{A(\beta)}\ \int_{-\infty}^0\Big[(t-s)^{%
\beta-1/2}-(-s)^{\beta-1/2}\Big]dW(s)+
\int_{0}^t(t-s)^{\beta-1/2}dW(s),
\end{eqnarray*}
where $W(s)$ is a standard Brownian motion and
\begin{equation*}
A(\beta)=\Big (\frac 1{2\beta}+\int_{0}^{\infty}\Big[(1+s)^{\beta-1/2}-s^{%
\beta-1/2}\Big]^2ds\Big)^{1/2}.
\end{equation*}




\medskip
{\bf Example 3.2.} Let $x_{k,n}=x_k/d_n$, where $x_k$ is defined as in Assumption {\bf \color{red} 2.1} and $d_n$ is defined as in (\ref {eqn:2:sec2.f2}).
Then $x_{k,n}$ satisfies Assumptions 3.2 and 3.3 with
\be
 G(t) &=&\begin{cases}
 W_{\mu - 3/2}(t),  & \mbox{under C1,} \\
W(t), & \mbox{under C2.}
\end{cases} \la {eqn:2:im19}
\ee
The verification of $x_{k,n}$ satisfying Assumption \ref{assump:2:weakConvergence} and \ref{assump:2:density} are largely similar to that the proof of Corollary 2.2 of \cite{wangphillips2010a}. The only additional work is to check (\ref{77}), {\bf \color{red} which is given in Appendix}.


\section{Proofs of main results}
This section provides proofs of the main results. We start with a lemma, which will be heavily used in the proof of main results.
Throughout this section, we denote constants by $C, C_1, C_2,...$, which may be different at each appearance.


\begin{lem} \la {lem1} For any real function $l(x)$ satisfying $\sup_x|l(x)|<\infty$ and $\int_{-\infty}^{\infty}|l(x)|dx<\infty$, there exist a constant $H_0$ not depending  on $t_1, t_2, t_3$ and $m$  such that
\be
&& \sup_x\, E\big(|\sum_{k=t_2}^{t_3}l[c_n\, (x_{k,n}+x)]|^m\mid {\mathcal F}_{n,t_1}\big) \no\\
&\le &  H_0^m \, (m+1)!\, n^d\,c_n^{-1}  (t_3-t_1)^{1-d}\big[1+ \big\{(t_3-t_2)^{1-d} n^d\,c_n^{-1}\big\}^{m-1}  \big]. \la {lm90}
\ee
for all $0\le t_1<t_2<t_3\le n$ and integer $m\ge 1$. In particular, by letting $t_1=0, t_2=1$ and $t_3=n$, we have
\be
 \sup_x\, E|\sum_{k=1}^{n}l[c_n\, (x_{k,n}+x)]|^m
&\le & H_0^m \, (m+1)!\, (n/c_n)^{m} . \la {eqn:2:lm91}
\ee

\end{lem}

{\it Proof.} First recall that, given on ${\mathcal F}_{s,n}$, $(x_{t,n}-x_{s,n})/d_{t,s,n}$
has a density $h_{t,s,n}(x)$ which is uniformly bounded by a constant $K$. Simple calculations show that, for $1\le s<t\le n$,
\be \E\big\{|l[c_n(x_{t,n}+x)]|\mid {\mathcal F}_{s,n}\big\}
&=&
\int_{-\infty}^{\infty}|l[c_n d_{t, s,n}\, y + c_n (x_{s, n}+x)] |h_{t, s,n}(y)dy\no\\
&\le& \frac {K}{c_n d_{t,s,n}}
\int_{-\infty}^{\infty}|l[y+  c_n (x_{s, n}+x)]|dy \no\\
&\le & K\,l_1  /(c_n d_{t,s,n}), \la {eqn:2:109}\ee
where $l_1=\int_{-\infty}^{\infty}|l(x)|dx$.
By virtue of this estimate, it follows from  conditional arguments repeatedly  that, for any $t_2\le k_1<k_2<...<k_m\le t_3$,
\bestar
&& \E \Big(\big| l[ c_n (x_{k_1,n} +x) ]\,...\, l[ c_n (x_{k_m,n} +x)]\big|\mid {\mathcal F}_{n,t_1}\Big) \no\\
&\le& \E \Big(\big| l[ c_n (x_{k_1,n} +x) ]\,...\,l[ c_n (x_{k_{m-1},n} +x)]\big|\,   \E \big(|l[ c_n (x_{k_m,n} +x)]|\mid {\mathcal F}_{n,k_{m-1}}\big)\mid {\mathcal F}_{n,t_1}\Big) \no\\
&\le& K\,l_1 \, c_n^{-1}\,  d_{k_m,k_{m-1},n}^{-1}\, \E \Big(\big| l[ c_n (x_{k_1,n} +x) ]\,...\,l[ c_n (x_{k_{m-1},n} +x)]\big|\,   \mid {\mathcal F}_{n,t_1}\Big) \no\\
&\le& ...... \no\\
&\le& (K\,l_1 )^m\, c_n^{-m}\, { d_{k_1,t_1,n}^{-1}}\,d_{k_2,k_1,n}^{-1}\cdots\, d_{k_m,k_{m-1},n}^{-1}.
\eestar
Therefore, by recalling $d_{t,s,n}\sim C_0\,[(n/(t-s)]^{-d}$ for some $0< d<1$ and letting $H_0=\max\{1,  K\,l_0\, l_1\,C_0 \}$ where $l_0=\sup_x|l(x)|$, we have
\bestar
&& \E\Big(|\sum_{k=t_2}^{t_3}l[c_n\, (x_{k,n}+x)]|^m \mid {\mathcal F}_{n,t_1}\Big) \no\\
&\le&  \max\{1, l_0^{m-1}\} \Big [ \sum_{k_1 =t_2}^{t_3}\E\Big( |l\big[ c_n(x_{t,n} +x) \big ] | \mid {\mathcal F}_{n,t_1}\Big)\no\\
&&  + 2\, \sum_{t_2 \le k_1 < k_2 \le t_3} \E\Big( |l\big[ c_n (x_{k_1,n} +x) \big ] l\big[c_n (x_{k_2,n} +x) \big ] | \mid {\mathcal F}_{n,t_1}\Big)+ ... \no\\
&&+ m!\, \sum_{t_2 \le k_1 < ... < k_m \le t_3} \E\Big( |l\big[ c_n (x_{k_1,n} +x) \big ]...\, l\big[ c_n (x_{k_m,n} +x) \big ]| \mid {\mathcal F}_{n,t_1}\Big) \Big ]\no\\
&\le&   H_0^m\,
\Big [ n^d\,c_n^{-1} \sum_{k_1 =t_2}^{t_3} (k_1-t_1)^{-d}\no\\
&& \qquad \qquad  + 2\,n^{2d}\,c_n^{-2} \sum_{t_2 \le k_1 < k_2 \le t_3}(k_1-t_1)^{-d}(k_2-k_1)^{-d} + ... \no\\
&& \qquad \qquad + m!\,n^{md}\,c_n^{-m} \sum_{t_2 \le k_1 < ... < k_m \le t_3} (k_1-t_1)^{-d}(k_2-k_1)^{-d}...(k_m-k_{m-1})^{-d}\Big ]\no\\
&\le& H_0^m \, (m+1)!\, n^d\,c_n^{-1}  (t_3-t_1)^{1-d}\big[1+ \big\{(t_3-t_2)^{1-d} n^d\,c_n^{-1}\big\}^{m-1}  \big].
\eestar
 This proves Lemma \ref {lem1}.  $\Box$


\vskip 0.5cm
We are now ready to prove the main results.

{\bf Proof of Theorem \ref {thmMainUpper}.} Let
 \be y_j=-[n^{m_0}]-1+j\,/ m_n',\quad  j=0, 1,2,...,\,m_n, \la {eqn:2:m1a}
\ee where $m_n'=[(n/c_n)^{1/2} c^2_n]$ and $m_n=2([n^{m_0}]+1)m_n'$. It follows that
\be
\sup_{|x|\le n^{m_0}}\big|S_n(x)\big|
&\le&  \max_{0 \le j \le m_n -1} \sup_{x \in [y_j, y_{j+1}]} \sum_{t =1}^n \big |g[c_n (x_{t,n} + x)]-g[c_n (x_{t,n} + y_j)]\big| \no\\
&\quad& + \max_{0 \le j \le m_n} \big |\sum_{t =1}^n g[c_n (x_{t,n} + y_j)]\big|  \no\\
&:=& \lambda_{1n} + \lambda_{2n}. \la {eqn:2:78}
\ee
It follows from Assumption \ref{assump:2:lipschitz} that
\be
 \lambda_{1n} &\le& C\, n\, c_n\max_{0 \le j \le m_n - 1} | y_{j+1} - y_{j}| = O\big [ (n/c_n)^{1/2}\big ],\la {eqn:2:m18a}
\ee
which yields $\lambda_{1n}=O(n/c_n),$ a.s., as $n/c_n\to \infty.$

We use Lemma \ref {lem1} to estimate $\lambda_{2n}$. To this end, let $m=\log n$ in (\ref {eqn:2:lm91}).
It follows from Markov inequality and the Stirling approximation of $(m+1)!$ that, for any $M_0\ge e^{m_0+3}H_0$ where $H_0$ is given as in Lemma \ref {lem1},
\begin{align}\la{p8}
& P \Big ( \max_{1 \le j \le m_n} \Big|\sum_{t = 1}^n g\big[ c_n(x_t +y_j)\Big| \big ] \ge M_0\, (n/c_n)\, \log n,\, i.o. \Big ) \no\\
&\le \lim_{s \to \infty} \sum_{n = s}^\infty \sum_{j = 1}^{m_n} P \Big ( \Big | \sum_{t = 1}^n g\big[ c_n(x_{t,n} +y_j) \big ] \Big |^m \ge \big [ M_0\, (n / c_n)\, \log n \big ]^{m} \Big ) \no\\
& \le  \lim_{s \to \infty} \sum_{n = s}^\infty \frac{ m_n }{[ M_0\, (n / c_n)\, \log n \big ]^m} \max_{1 \le j \le m_n} \E \Big | \sum_{t = 1}^n g\big[ c_n(x_t +y_j) \big ]\Big |^m \no\\
& \le \lim_{s \to \infty} \sum_{n = s}^\infty \frac{ m_n \, H_0^m (m+1)!}{[ M_0\,  \log n \big ]^m} \no\\
& \le C\,\lim_{s \to \infty} \sum_{n = s}^\infty \frac{ n^{m_0+1}\, H_0 ^m}{ \big [ M_0\,  \log n \big ]^m}   \sqrt{2\pi\,(m+1)}\, \Big ( \frac{m+1}{e } \Big )^{m+1} \no\\
& \le C\,\lim_{s \to \infty} \sum_{n = s}^\infty   e^{- (m_0+3) \log n}\, n^{m_0+1}\, \log n\,  \no\\
& \le C_1 \lim_{s \to \infty} \sum_{n = s}^\infty n^{-2}  = 0.
\end{align}
This proves $\lambda_{2n}=O[(n/c_n)\log n],$ a.s. Taking the estimates of $\lambda_{1n}$ and $\lambda_{2n}$ into (\ref {eqn:2:78}), we obtain the required (\ref {eqn:2:ad1}).

To prove (\ref {eqn:2:69}), we first write
\begin{align}
\sum_{k=1}^{n}g[c_{n}\,(x_{k,n}+x)] &= \sum_{k=1}^{n}g[c_{n}\,(x_{k,n}+x)] I(|x_{t,n}| \le n^{m}/2)  \no\\
&\qquad + \sum_{k=1}^{n}g[c_{n}\,(x_{k,n}+x)] I(|x_{t,n}| > n^{m}/2) \no\\
&:= \lambda_{1n}(x) + \lambda_{2n}(x) \la {eqn:2:71}
\end{align}
It follows from (\ref {eqn:2:ad1}) and $n \sup_{|x| > n^{m}}|g(c_n \, x)| = O[(n/c_n) \log n]$ that
\begin{align}
\sup_{x\in R} |\lambda_{1n}(x) |&\le \sup_{|x| \le n^{m}} |\lambda_{1n}(x)| + \sup_{|x| > n^{m}} | \lambda_{1n}(x) | \no\\
&\le O[(n/c_n)\log n] + n \, \sup_{|x| > n^{m}/2} | g(c_n \, x)| \quad a.s.  \no\\
&\le O[(n/c_n)\log n] \quad a.s., \la {eqn:2:72}
\end{align}
As for $\lambda_{2n}(x)$, we have
\begin{align}
\sup_{x\in R} |\lambda_{2n}(x)| &\le C \sum_{t = 1}^n  I(| x_{t,n}| > n^{m}/2)  \le C n^{-m k/2} \sum_{t = 1}^n  |x_{t,n}|^{k} \no\\
& = O(1) \quad a.s., \la {eqn:2:73}
\end{align}
Combining (\ref {eqn:2:71})-(\ref {eqn:2:73}), we prove (\ref {eqn:2:69}).
The proof of Theorem \ref {thmMainUpper} is now complete. $\Box$

\medskip
{\bf Proof of Theorem \ref {thm:2:MainLower}.}  It follows immediately from Theorem \ref {thm:2:thm7} and Remark 3.2. We omit the detalis.

 \medskip
 {\bf Proof of Theorem \ref {thm:2:thm7}.}
Let $\eta_n=(n/c_n)\log^{-\lambda}n,$ $b_n = [n \log^{-\nu} n]$, where $\nu=(\lambda+1)/(1-d)$, and let $T_n$ be the largest integer $s$ such that $s b_n \le n$. Also write  $y_j = -M_0/\log^\gamma n +  j/  m_n',  j = 0,1,2,...,m_n,$ where $m'_n = [(n/c_n)^{1/2} c_n^2]$ and $m_n= [M_0m_n'/\log^\gamma n]+1$. It is readily seen that
\be
n/b_n \sim  \log^\nu n, \quad T_nb_n\le n, \quad m_n\le Cn^2, \la {eqn:2:110}
\ee
due to $c_n\to\infty$ and $c_n/n\to 0$. Furthermore, by recalling the definitions of $\lambda$ and $\eta$, tedious but elementary calculations show that, whenever $\gamma\ge 0$,
\be
d\nu-\eta \le 1-2\lambda, \quad (d-1)\nu+\lambda\le -1, \quad 2d\nu-\gamma\le 1-\lambda. \la {eqn:2:112}
\ee


We now return to the proof of Theorem \ref {thm:2:thm7}. Using the similar arguments as in the proof of (\ref {eqn:2:78}),  we have
\be
&& \sup_{|x|\le M_0/\log^\gamma n} | S_n(x)-S_n(0)| \no\\
&\le & \sup_{|x|\le M_0/\log^\gamma n} | S_n(y_j)-S_n(0)|+O_{a.s.}[( n/c_n)^{1/2}] \no\\
&\le& \max_{1\le j\le m_n}| \sum_{s=2}^{T_n-1} \Delta_{ns}(y_j)|+\max_{1\le j\le m_n}\Delta_n(y_j)+ O_{a.s.}[( n/c_n)^{1/2}],  \la {eqn:2:95}
\ee
where, for  $s = 1,..., T_n$ ,
\bestar
\Delta_{ns}(x) &=& \sum_{t = sb_n + 1}^{(s+1)b_n} \big ( g[c_n( x_{n,t}+x)]  - g(c_n\, x_{n,t}) \big ) ,\no\\
\Delta_n(x) &\le & \Big(\sum_{t =  1}^{2b_n} +\sum_{t =  T_nb_n}^{n}\Big)\,\big | g[c_n( x_{n,t}+x)]  - g(c_n\, x_{n,t}) \big |.
\eestar
Recall $\eta_n=(n/c_n)\log^{-\lambda}n$. Using Theorem 3.1, it is readily seen that
\bestar
\max_{1\le j\le m_n}\Delta_n(y_j)  &\le& C\big[(b_n + |n-T_nb_n|)/c_n\big]\log n \no\\
&\le& C\, (n/c_n)\, \log^{1-\nu} n
\le C\, \eta_n\, \log n,\quad a.s.
\eestar
This, together with (\ref {eqn:2:95}), implies that (\ref {eqn:2:MainLower1}) will follow if we prove
\be
\max_{1\le j\le m_n}\Big(| \sum_{\substack{s = 2 \\ s \in even}}^{T_n} \Delta_{ns}(y_j)|+| \sum_{\substack{s = 2 \\ s \in odd}}^{T_n} \Delta_{ns}(y_j)|\Big)  &=& O_P (\eta_n \, \log n).  \la {eqn:2:21}
\ee
We only prove (\ref {eqn:2:21}) for $s\in even$. The other is similar and hence the details are omitted.
To this end, let $\F_{n, v}^*= \F_{n, (2v+1)b_n}, v\ge 0$, and $M_1 > 0$ is chosen later,
\bestar
\Delta_{ns}'(x) &=& \Delta_{n,2s}(x)I(|\Delta_{n, 2s}(x)|\le M_1\, \eta_n ), \no\\
 \Delta_{n s}^*(x) &=& \Delta_{n, s}'(x)- \E \big(\Delta_{n, s}'(x)\mid \F_{n, s-1}^*\big).
\eestar
Under these notation, to prove (\ref {eqn:2:21}) for $s\in even$, it suffices to show
\be \lam_{1n} &:=&\max_{1\le j\le m_n}| \sum_{s=1}^{T_n/2} \Delta_{ns}^*(y_j)|
=O_P (\eta_n \, \log n), \la {eqn:2:22} \\
\lam_{2n} &:=& \max_{1\le j\le m_n}| \sum_{s=1}^{T_n/2} \E \big(\Delta_{n, 2s}(y_j)\mid \F_{n, s-1}^*\big)|
=O_P (\eta_n \, \log n), \la {eqn:2:23}\\
\lam_{3n} &:=& \max_{1\le j\le m_n}| \sum_{s=1}^{T_n/2} \Big(\Delta_{n, 2s}(y_j)I(|\Delta_{n, 2s}(y_j)|> M_1\, \eta_n )\no\\
&& \qquad\qquad +
\E \Big[\Delta_{n, 2s}(y_j)I(|\Delta_{n, 2s}(y_j)|> M_1\, \eta_n)\mid \F_{n, s-1}^*\Big] \Big) \no\\
&=& O_P (\eta_n \, \log n). \la {eqn:2:24}
 \ee

 We start with (\ref {eqn:2:23}).  Note that, for any $2sb_n<t\le (2s+1)b_n$ and $|x|\le M_0/\log^\gamma n$ (letting $s_n=(2s-1)b_n$),
 \begin{align} \la{24.5}
& \Big| E\big [ g[c_n (x_{t,n} + x)] - g( c_n\, x_{t,n}) \Big | \F^*_{n, s-1} \big ]\Big|= \Big| E\big [ g[c_n (x_{t,n} + x)] - g( c_n\, x_{t,n}) \Big | \F_{n, s_n} \big ]\Big|\no\\
&\quad = \Big| \int_{-\infty}^{\infty} \Big (g[c_n (x_{s_n,n} + d_{t, s_n, n} y + x)] - g[ c_n\, (x_{s_n,n} + d_{t,s_n,n} y) ] \Big ) \, h_{t, s_n,n}(y) dy \Big|\no\\
&\quad \le d_{t,s_n,n} ^{-1}\,
 \int_{-\infty}^{\infty}  g[c_n(y+x_{s_n,n})]\big | h_{t, s_n,n}[(y - x)/ d_{t,s_n,n}] -  h_{t, s_n,n}(y/ d_{t,s_n,n}) \big |\,dy \no\\
 &\quad \le C\,  c_n^{-1}d_{t,s_n,n} ^{-1}\min\{|x|d_{t,s_n,n} ^{-1}, 1\} \le C\,|x|\, c_n^{-1} (n/b_n)^{2d}\, \no\\
 &\quad \le C\,  c_n^{-1}  \log^{2 d\nu-\gamma}n,
\end{align}
due to Assumption \ref{assump:2:density} and $d_{t,s,n}\sim C_0[n/(t-s))]^{-d}$. It is readily seen that
\be \la{25}
\lam_{2n} &\le& \sum_{s=1}^{T_n/2}  \max_{1\le j\le m_n} |\E \big(\Delta_{n, 2s}(y_j)\mid \F_{n, s-1}^*\big)| \no\\
&\le& C\, (n/c_n)\, \log^{2d\nu-\gamma}n = O_P(\eta_n \, \log n),
\ee
due to (\ref {eqn:2:112}),
which yields (\ref {eqn:2:23}).

Next for (\ref {eqn:2:24}). Using Lemma \ref {lem1} with $t_1=0, t_2=2sb_n+1$ and $t_3=(2s+1)b_n$,  for any integer $m\ge 1$,
\bestar
\sup_x \E |\Delta_{n, 2s}(x)|^m &\le& H_0^m (m+1)!\, (n/c_n)\, \big\{1+ \big [(n/c_n) (n/ b_n)^{d-1} \big ]^{m-1}\big\}\no\\
&\le& 2H_0^m (m+1)! (n/c_n)^m (n/b_n)^{(d - 1)(m-1)},
\eestar
whenever $(n/c_n)\log^{-(\lambda+1)}n\to\infty$. By virtue of this fact, we have
\bestar
E\lam_{3n} &\le& 2\,\sum_{j=1}^{m_n}\,
\sum_{s=1}^{T_n/2}\E \Delta_{n, 2s}(y_j)I(|\Delta_{n, 2s}(y_j)|> M_1\, \eta_n) \no\\
&\le& 2\, m_n T_n \,\, H_0^m (m+1)!  (n/c_n)\Big [ \frac{(n/c_n)(n/b_n)^{d-1}}{M_1\,\eta_n} \Big ]^{m-1} \no\\
&\le& C\, n^4 (H_0/M_1)^m\, (m+1)!   \log^{-(m-1)} n,
\eestar
due to (\ref {eqn:2:110}) and $(d-1)\nu+\lambda\le -1$ by (\ref {eqn:2:112}).
Taking $m = \log n$ and letting $M_1 \ge 5H_0$,
it follows from the Stirling approximation of $(m+1)!$ that
\be \la{26}
E\lam_{3n} &\le& C n^4 \log^5 n \exp \{-(M_1/H_0) \, \log n\} \le Cn^{-1}\log^5 n \to 0,
\ee
which implies that $\lam_{3n}=o_P(1)$. Hence (\ref {eqn:2:24}) follows.

We finally consider (\ref {eqn:2:22}). First note that, similarly to the proof of (\ref {eqn:2:24.5}),
\bestar
I_{k,j}&:=&\Big|\E \Big(  \{g[c_n(x_{n,j} + x)] - g[c_n x_{n,j}] \}\mid {\mathcal F}_{n, k}\Big)\Big| \no\\
&\le& d_{j,k, n}^{-1}\,
 \int_{-\infty}^{\infty} g[c_n(x_{n,k} + y)]
 \big | h_{j, k, n}[(y - x)/ d_{j,k, n}] -  h_{j,k,n}( y/ d_{j,k,n}) \big |\,dy \no\\
 & \le& C\, c_n^{-1}\,  d_{j,k, n}^{-1} \min\{|x| d_{j,k, n}^{-1}, 1\}\no\\
 &\le & C\, c_n^{-1}\, [n/(j-k)]^d \min\{|x|[n/(j-k)]^{d},1\},
\eestar
for any $k<j$. This, together with  (\ref {eqn:2:109}), implies  that, for any $|x| \le M_0/\log^{\gamma}n$,
\begin{align}
&\quad E[\Delta_{ns}^{*2}(x) | \F^*_{n, s-1}] \le 2 E[ \Delta_{n, 2s}^2(x) | \F_{n, (2s-1) b_n} ] \no\\
&\le \sum_{k = 2sb_n +1}^{(2s+1)b_n} E \Big (  \{ \, g[c_n(x_{n,t} + x)] - g[c_n x_{n,t}]\, \}^2 \,| \,\F_{n, (2s-1)b_n} \Big ) \no\\
&\quad + 2 \sum_{2sb_n + 1 \le k < j \le (2s+1)b_n} \big|E \Big ( \{g[c_n(x_{n,k} + x)] - g[c_n x_{n,k}] \} \, \no\\
&\hskip 5cm \{g[c_n(x_{n,j} + x)] - g[c_n x_{n,j}] \}\,\Big | \F_{n,(2s-1)b_n} \Big ) \big| \no\\
&\le C\, (n/c_n) (n /b_n)^{d-1}  + 2 \sum_{2sb_n + 1 \le k < j \le (2s+1)b_n}
E \Big ( |g[c_n(x_{n,k} + x)] - g[c_n x_{n,k}] |\,  |I_{k,j}|\, \Big  | \,\F_{n, (2s-1)b_n} \Big )   \no\\
&\le C\, (n/c_n) (n /b_n)^{d-1} + C\,n^{2d}\, c_n^{-2}\,b_n^{-d}  \sum_{ 2sb_n + 1\le k<j\le (2s+1)b_n}\,(j-k)^{-d}\min\{n^d\log^{-\gamma} n (j-k)^{-d}, 1\} \no\\
&\le C\, (n/c_n) (n /b_n)^{d-1} + C\,n^{2d}\, c_n^{-2}\,b_n^{1-d}\sum_{k=1}^{b_n} k^{-d} \min\{(n/k)^d\log^{-\gamma}n, 1\} \no\\
&\le C\, (n/c_n) (n /b_n)^{d-1} \big[1+   \, (n/c_n)  \log^{-\eta}n\big],
\end{align}
where we have used the fact: for $0<d<1$, letting $\zeta = \gamma/d$,
\bestar
&& \sum_{k=1}^{b_n} k^{-d} \min\{(n/k)^d\log^{-\gamma}n, 1\} \no\\
&\le& \sum_{k=1}^{n/\log^{\zeta} n} k^{-d}+ n^d\log^{-\gamma}n\sum_{k=n/\log^{\zeta} n+1}^{b_n} k^{-2d}\no\\
&\le& C\, n^{1-d}\, \log^{-\eta}n.
\eestar
and $\eta$ is given in (\ref{eta}). It follows from this estimate that
\bestar
&& \max_{0\le j\le m_n}\, \sum_{s=1}^{T_n/2}\,\E [\Delta_{ns}^{*2}(y_j)\mid {\mathcal F}_{n, s-1}^*] \no\\
&\le&  C\, (n/c_n) (n /b_n)^{d}\big[1+   \, (n/c_n)  \log^{-\eta}n\big]\no\\
&\le&C(n/c_n)^2\,  \log^{d\nu-\eta}n \le
C\, \eta_n^2 \log n,
\eestar
due to (\ref {eqn:2:112}) and $(n/c_n)\log^{-\eta}n\to \infty$.
This, together with the facts that  $|\Delta_{ns}^{*}(y_j)|\le \eta_n$ and for each $j$,
$\{\Delta_{ns}^{*}(y_j), {\mathcal F}_{n, s}^*\}$ forms a martingale difference, it follows from
the well-known martingale exponential inequality
(see, e.g., \cite{delapena1999}) that, there exists a $M_0\ge 3$ such that, as $n \to \infty$,
\be
&& P[\lam_{1n} \ge  M_0 \eta_n\, \log n] \no\\
&\le&
 P\Big[\lam_{1n} \ge  M_0 \eta_n \, \log n,\ \
 \max_{0\le j\le m_n}\, \sum_{s=1}^{T_n/2}\,\E [\Delta_{ns}^{*2}(y_j)\mid {\mathcal F}_{n, s-1}^*]\le C\, \eta_n^2\, \log n  \Big] + o(1)\no\\
 &\le& \sum_{j=0}^{m_n} P\Big[\sum_{s=1}^{T_n/2} \Delta_{ns}^*(y_j)\ge M_0 \eta_n\, \log n, \ \
 \sum_{s=1}^{T_n/2}\,\E [\Delta_{ns}^{*2}(y_j)\mid {\mathcal F}_{n, s-1}^*]\le C\,\eta_n^2 \, \log n \Big] + o(1) \no\\
 &\le&m_n\, \exp\Big\{-\frac {M_0^2 \,\log^2 n} {2C\log n+2M_0\log n} \Big \} + o(1) \no\\
 &\le&m_n\, \exp \{-M_0\log n \} + o(1) \to 0, \la {eqn:2:p10}
\ee
where the last inequality follows from (\ref {eqn:2:110}).
 This yields $\lam_{1n}=O_P\big( \eta_n \, \log n \big )$.
Combining (\ref {eqn:2:25})-(\ref {eqn:2:p10}), we establish (\ref {eqn:2:21}).
 $\Box$



% ------------------------------------------------------------------------

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
